{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6390c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flashdiv.flows.egnn_new.egnn_var import EGNN_dynamics as EGNN_dynamics_var\n",
    "#from flashdiv.flows.egnn_new.egnn import EGNN_dynamics_Noe\n",
    "from flashdiv.flows.egnn_new.egnn import EGNN_dynamics_Noe\n",
    "\n",
    "import math\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81368481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_equivariance(model, n_particles):\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    #torch.manual_seed(1)  # For reproducibility\n",
    "\n",
    "    n_batch = 20\n",
    "\n",
    "    # Generate random positions for 13 particles in 3D\n",
    "    xs = torch.randn(n_batch, n_particles, 2, device=device)\n",
    "    t = torch.randn(n_batch, device=device)  # Random time input\n",
    "\n",
    "    # --- Rotation Equivariance Test ---\n",
    "    theta = math.pi / 3  # 60 degrees rotation\n",
    "    R = torch.tensor([[math.cos(theta), -math.sin(theta)],\n",
    "                      [math.sin(theta),  math.cos(theta)]], dtype=torch.float32, device=device)\n",
    "    xs_rot = torch.matmul(xs, R.T)\n",
    "\n",
    "    vel = model(xs, t)\n",
    "    vel_rot = model(xs_rot, t)\n",
    "    vel_rot_expected = torch.matmul(vel, R.T)\n",
    "\n",
    "    err_rotation = (vel_rot - vel_rot_expected).abs().max().item()\n",
    "    print(f\"[Rotation equivariance error]: {err_rotation:.6e}\")\n",
    "\n",
    "    # --- Permutation Equivariance Test ---\n",
    "    perm = torch.randperm(n_particles, device=device)\n",
    "    xs_perm = xs[:, perm, :]\n",
    "    vel_perm = model(xs_perm, t)\n",
    "    vel_expected = vel[:, perm, :]\n",
    "\n",
    "    err_perm = (vel_perm - vel_expected).abs().max().item()\n",
    "    print(f\"[Permutation equivariance error]: {err_perm:.6e}\")\n",
    "\n",
    "    # --- Center of Mass Preservation Test ---\n",
    "    com_input = xs.mean(dim=1)  # should be (0, 0)\n",
    "    com_output = vel.mean(dim=1)  # check output COM\n",
    "    err_com = com_output.norm(dim=-1).max().item()\n",
    "    print(f\"[Center of mass error]: {err_com:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d36d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing custom EGNN_dynamics\n",
      "[Rotation equivariance error]: 4.768372e-07\n",
      "[Permutation equivariance error]: 7.531881e-03\n",
      "[Center of mass error]: 2.716146e-02\n",
      "Initializing custom EGNN_dynamics\n",
      "[Rotation equivariance error]: 7.152557e-07\n",
      "[Permutation equivariance error]: 5.364418e-07\n",
      "[Center of mass error]: 4.368677e-02\n"
     ]
    }
   ],
   "source": [
    "n_particles = 13\n",
    "dim = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EGNN_dynamics_Noe(n_particles=n_particles-1,n_dimension=dim, device=device, out_node_nf=12)\n",
    "model = model.to(device)\n",
    "test_equivariance(model, n_particles)\n",
    "\n",
    "model2 = EGNN_dynamics_var(n_particles=n_particles,n_dimension=dim, device=device)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "test_equivariance(model2, n_particles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b890cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from flashdiv.flows.trainer import FlowTrainer, DistillationTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat, reduce, einsum\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from flashdiv.flows.eqtf_com import EqTransformerFlowSherryVariation\n",
    "from flashdiv.flows.trainer import FlowTrainer\n",
    "from flashdiv.flows.egnn_et import EasyTrace_EGNN\n",
    "from flashdiv.flows.etvf import EasyTraceVelocityField\n",
    "from flashdiv.flows.eqtf_pair import EqTransformerFlow as EqTransformerFlow_pair\n",
    "from flashdiv.flows.eqtf_gen import EqTransformerFlow as EqTransformerFlow\n",
    "from flashdiv.flows.new_vf import EqTransformerFlow as NewEqTransformerFlow\n",
    "from flashdiv.flows.egnn_new.egnn import EGNN_dynamics_Noe\n",
    "from flashdiv.flows.egnn import EGNN_dynamics\n",
    "\n",
    "\n",
    "\n",
    "from types import SimpleNamespace    # lightweight stand-in for argparse.Namespace\n",
    "\n",
    "\n",
    "#plt.style.use('my_style')\n",
    "#use default matplotlib style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Example scripe\n",
    "\n",
    "plt.rcParams.update({'text.usetex': True,\n",
    "                     'font.family': 'CMU',\n",
    "                     'text.latex.preamble': r'\\usepackage{amsfonts}'})\n",
    "nbparticles = 13\n",
    "dim = 3\n",
    "\n",
    "def parse_args(args):\n",
    "    if args.nn == 'egnn':\n",
    "        velocitynet = EGNN_dynamics(n_particles=nbparticles, n_dimension=dim, device=device, hidden_nf=int(args.gnn_hidden_dim),\n",
    "        act_fn=torch.nn.SiLU(), n_layers=int(args.nb_layers), recurrent=True, tanh=True, attention=True, agg='sum')\n",
    "    elif args.nn == 'egnn_et':\n",
    "        velocitynet = EasyTrace_EGNN(n_particles=nbparticles, hidden_nf=int(args.gnn_hidden_dim),\n",
    "            act_fn=torch.nn.SiLU(), n_layers=int(args.nb_layers), recurrent=True, tanh=True, attention=True, agg='sum')\n",
    "    elif args.nn == \"etvf\":\n",
    "        velocitynet = EasyTraceVelocityField(n_particles=nbparticles,gnn_hidden_dim = int(args.gnn_hidden_dim),\n",
    "                                            tf_hidden_dim = int(args.tf_hidden_dim),act_fn=torch.nn.SiLU(), n_layers=int(args.nb_layers), \n",
    "                                            recurrent=True, tanh=True, attention=True, agg='sum')\n",
    "    elif args.nn == \"eqtf_sherry\":\n",
    "        velocitynet = EqTransformerFlowSherryVariation(\n",
    "            input_dim=3,\n",
    "            embed_dim=int(args.tf_hidden_dim))\n",
    "    elif args.nn == \"eqtf_pair\":\n",
    "        velocitynet = EqTransformerFlow_pair(\n",
    "            n_particles=nbparticles,\n",
    "            hidden_nf=int(args.tf_hidden_dim))\n",
    "    elif args.nn == \"eqtf\":\n",
    "        velocitynet = EqTransformerFlow(\n",
    "        n_particles=nbparticles,\n",
    "        hidden_nf=int(args.tf_hidden_dim))\n",
    "    elif args.nn == \"new_vf\":\n",
    "        velocitynet = NewEqTransformerFlow(\n",
    "        n_particles=nbparticles,\n",
    "        hidden_nf=int(args.tf_hidden_dim),\n",
    "        gnn_hidden_nf=int(args.gnn_hidden_dim),\n",
    "        )\n",
    "    elif args.nn == \"egnn_noe\":\n",
    "        velocitynet = EGNN_dynamics_Noe(\n",
    "        n_particles=nbparticles - 1,\n",
    "        device=device,\n",
    "        n_dimension=dim,\n",
    "        hidden_nf=12,\n",
    "        act_fn=torch.nn.SiLU(),\n",
    "        n_layers=2,\n",
    "        recurrent=True,\n",
    "        tanh=True,\n",
    "        attention=True,\n",
    "        condition_time=True,\n",
    "        # in_node_nf=1,  # 1 for time, 2 for position\n",
    "        out_node_nf=12, # expressivity for potential\n",
    "        )\n",
    "    elif args.nn == \"egnn_var\":\n",
    "        velocitynet = EGNN_dynamics_var(\n",
    "        n_particles=nbparticles,\n",
    "        device=device,\n",
    "        n_dimension=dim,\n",
    "        hidden_nf=12,\n",
    "        act_fn=torch.nn.SiLU(),\n",
    "        n_layers=2,\n",
    "        recurrent=True,\n",
    "        tanh=True,\n",
    "        attention=True,\n",
    "        condition_time=True,\n",
    "        # in_node_nf=1,  # 1 for time, 2 for position\n",
    "        out_node_nf=12, # expressivity for potential\n",
    "        )\n",
    "    return velocitynet\n",
    "\n",
    "def load_model(nn,ckpt_path):\n",
    "    args = SimpleNamespace(\n",
    "        nn=nn,            # pick one of: \"egnn\", \"etvf\", \"eqtf_sherry\", \"eqtf\"\n",
    "        gnn_hidden_dim=32,\n",
    "        tf_hidden_dim=256,\n",
    "        nb_layers=4\n",
    "    )\n",
    "\n",
    "    velocitynet = parse_args(args)\n",
    "    ckpt = torch.load(ckpt_path, map_location='cuda')\n",
    "\n",
    "    # Assume `ckpt` is a raw state_dict, or nested inside a dict under \"state_dict\"\n",
    "    if 'state_dict' in ckpt:\n",
    "        state_dict = ckpt['state_dict']\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "\n",
    "    velocityTrainer = FlowTrainer(velocitynet)\n",
    "    velocityTrainer.load_state_dict(state_dict)\n",
    "    # # velociTrainer = FlowTrainer.load_from_checkpoint(flow_model = velocitynet) #\n",
    "    velocitynet = velocityTrainer.flow_model.to(device)\n",
    "    print(f\"nb params : {sum(p.numel() for p in velocityTrainer.parameters())}\")\n",
    "    return velocitynet\n",
    "\n",
    "def logprob0(x, scale):\n",
    "    \"\"\"\n",
    "    x : (batch, p, dim)\n",
    "    \"\"\"\n",
    "    return - reduce(x**2, 'b p d -> b', 'sum') / (2 * scale**2)\n",
    "\n",
    "def generate_samples_and_prob(model, batch_size=50, n_iterations=200):\n",
    "    xt = []\n",
    "    target_log_prob = []\n",
    "    x0_list = []\n",
    "    times = torch.linspace(0,1,2).to(device)\n",
    "    for k in range(n_iterations):\n",
    "        # x0 = gen_sorted_gaussian(batch_size, nbparticles, dim, noise_scale = 0.5).to(device)\n",
    "        x0 = torch.randn(batch_size, nbparticles, dim).to(device) * 1.0\n",
    "        x0 -= x0.mean(dim=1, keepdim=True)  # center the particles\n",
    "        with torch.no_grad():\n",
    "            xt_, target_log_prob_ = model.sample_logprob(x0, logprob0(x0, 1.0), times, div_method='direct_trace', method='rk4',\n",
    "            options = {\n",
    "                'step_size': 1 / 30,\n",
    "                # 'max_num_steps': int(1e3)\n",
    "                }\n",
    "            )\n",
    "            xt_ = xt_[-1]\n",
    "            target_log_prob_ = target_log_prob_[-1]\n",
    "        xt.append(xt_.detach())\n",
    "        target_log_prob.append(target_log_prob_.detach())\n",
    "        x0_list.append(x0.detach())\n",
    "    xt = rearrange(xt, 'l b p d -> (l b) p d')\n",
    "    target_log_prob = rearrange(target_log_prob, 'l b -> (l b)')\n",
    "    x0_list = rearrange(x0_list, 'l b p d -> (l b) p d')\n",
    "\n",
    "    return xt, target_log_prob, x0_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4109464",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124megnn_var\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/ssd/flow_matching/flash-div/0702_lj_13_reflow/flow_model_learning_rate_0.0001_batch_size_256_nb_epochs_60_gnn_hidden_dim_32_tf_hidden_dim_256_temp_1.0_nb_layers_4_nn_egnn_var/checkpoints/last-v1.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m egnn_var \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(nn_type,ckpt_path)\n\u001b[1;32m      5\u001b[0m test_equivariance(egnn_var, n_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#xt, target_log_prob, x0_list = generate_samples_and_prob(egnn_noe, batch_size=50, n_iterations=200)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "nn_type=\"egnn_var\"\n",
    "ckpt_path = \"/mnt/ssd/flow_matching/flash-div/0702_lj_13_reflow/flow_model_learning_rate_0.0001_batch_size_256_nb_epochs_60_gnn_hidden_dim_32_tf_hidden_dim_256_temp_1.0_nb_layers_4_nn_egnn_var/checkpoints/last-v1.ckpt\"\n",
    "egnn_var = load_model(nn_type,ckpt_path)\n",
    "test_equivariance(egnn_var, n_particles=13)\n",
    "#xt, target_log_prob, x0_list = generate_samples_and_prob(egnn_noe, batch_size=50, n_iterations=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a31fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rotation equivariance error]: 1.385808e-06\n",
      "[Permutation equivariance error]: 5.327601e-01\n",
      "[Center of mass error]: 2.084065e-01\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
